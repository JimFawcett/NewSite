<svg viewBox="0 0 1200 800" xmlns="http://www.w3.org/2000/svg">
  <!-- Define styles -->
  <defs>
    <style>
      .title { font-family: Arial, sans-serif; font-size: 24px; font-weight: bold; fill: #2c3e50; }
      .subtitle { font-family: Arial, sans-serif; font-size: 16px; font-weight: bold; fill: #34495e; }
      .label { font-family: Arial, sans-serif; font-size: 14px; fill: #2c3e50; }
      .small-label { font-family: Arial, sans-serif; font-size: 12px; fill: #555; }
      .box { fill: #ecf0f1; stroke: #34495e; stroke-width: 2; }
      .tokenizer-box { fill: #3498db; stroke: #2980b9; stroke-width: 2; }
      .embedding-box { fill: #9b59b6; stroke: #8e44ad; stroke-width: 2; }
      .attention-box { fill: #e74c3c; stroke: #c0392b; stroke-width: 2; }
      .neural-box { fill: #2ecc71; stroke: #27ae60; stroke-width: 2; }
      .output-box { fill: #f39c12; stroke: #d68910; stroke-width: 2; }
      .arrow { fill: none; stroke: #34495e; stroke-width: 2; marker-end: url(#arrowhead); }
      .neuron { fill: #3498db; stroke: #2980b9; stroke-width: 1.5; }
      .connection { stroke: #95a5a6; stroke-width: 1; opacity: 0.3; }
      .attention-line { stroke: #e74c3c; stroke-width: 2; opacity: 0.6; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#34495e" />
    </marker>
  </defs>
  
  <!-- Title -->
  <text x="600" y="30" class="title" text-anchor="middle">Large Language Model (LLM) Architecture</text>
  
  <!-- Input Text -->
  <text x="100" y="80" class="subtitle">Input Text</text>
  <rect x="80" y="90" width="240" height="50" class="box" rx="5"/>
  <text x="200" y="122" class="label" text-anchor="middle">"The cat sat on the"</text>
  
  <!-- Arrow to Tokenizer -->
  <path d="M 200 140 L 200 170" class="arrow"/>
  
  <!-- Tokenizer -->
  <text x="100" y="195" class="subtitle">1. Tokenizer</text>
  <rect x="80" y="205" width="240" height="90" class="tokenizer-box" rx="5"/>
  <text x="200" y="230" class="label" text-anchor="middle" fill="white">Breaks text into tokens</text>
  <text x="200" y="250" class="small-label" text-anchor="middle" fill="white">["The", "cat", "sat", "on", "the"]</text>
  <text x="200" y="270" class="small-label" text-anchor="middle" fill="white">Token IDs: [145, 2574, 3829, 319, 145]</text>
  
  <!-- Arrow to Embeddings -->
  <path d="M 200 295 L 200 325" class="arrow"/>
  
  <!-- Embeddings -->
  <text x="100" y="350" class="subtitle">2. Token Embeddings</text>
  <rect x="80" y="360" width="240" height="70" class="embedding-box" rx="5"/>
  <text x="200" y="385" class="label" text-anchor="middle" fill="white">Convert to vectors</text>
  <text x="200" y="405" class="small-label" text-anchor="middle" fill="white">[0.23, -0.45, 0.12, ... ] (e.g., 768 dims)</text>
  
  <!-- Arrow to Neural Network -->
  <path d="M 320 395 L 380 395" class="arrow"/>
  
  <!-- Neural Network Layers -->
  <text x="550" y="195" class="subtitle">3. Neural Network (Transformer Layers)</text>
  
  <!-- Layer visualization -->
  <rect x="400" y="205" width="380" height="400" class="neural-box" rx="5"/>
  
  <!-- Multi-Head Attention -->
  <text x="590" y="230" class="label" text-anchor="middle" fill="white" font-weight="bold">Multi-Head Attention</text>
  <rect x="420" y="240" width="340" height="120" class="attention-box" rx="5"/>
  
  <!-- Attention mechanism visualization -->
  <text x="590" y="265" class="small-label" text-anchor="middle" fill="white">Query (Q) • Key (K) • Value (V)</text>
  
  <!-- Show attention weights -->
  <g>
    <circle cx="470" cy="300" r="8" fill="white" opacity="0.9"/>
    <circle cx="530" cy="300" r="8" fill="white" opacity="0.9"/>
    <circle cx="590" cy="300" r="8" fill="white" opacity="0.9"/>
    <circle cx="650" cy="300" r="8" fill="white" opacity="0.9"/>
    <circle cx="710" cy="300" r="8" fill="white" opacity="0.9"/>
    
    <text x="470" y="320" class="small-label" text-anchor="middle" fill="white">The</text>
    <text x="530" y="320" class="small-label" text-anchor="middle" fill="white">cat</text>
    <text x="590" y="320" class="small-label" text-anchor="middle" fill="white">sat</text>
    <text x="650" y="320" class="small-label" text-anchor="middle" fill="white">on</text>
    <text x="710" y="320" class="small-label" text-anchor="middle" fill="white">the</text>
    
    <!-- Attention connections (showing "cat" attending to other tokens) -->
    <line x1="530" y1="300" x2="470" y2="300" class="attention-line" stroke-width="1"/>
    <line x1="530" y1="300" x2="590" y2="300" class="attention-line" stroke-width="3"/>
    <line x1="530" y1="300" x2="650" y2="300" class="attention-line" stroke-width="2"/>
    <line x1="530" y1="300" x2="710" y2="300" class="attention-line" stroke-width="1"/>
  </g>
  
  <text x="590" y="350" class="small-label" text-anchor="middle" fill="white">Attention scores determine relevance</text>
  
  <!-- Feed-Forward Network -->
  <text x="590" y="385" class="label" text-anchor="middle" fill="white" font-weight="bold">Feed-Forward Network</text>
  <rect x="420" y="395" width="340" height="80" class="box" rx="5"/>
  
  <!-- Simple neural network visualization -->
  <g>
    <!-- Input layer -->
    <circle cx="480" cy="425" r="6" class="neuron"/>
    <circle cx="480" cy="445" r="6" class="neuron"/>
    <circle cx="480" cy="465" r="6" class="neuron"/>
    
    <!-- Hidden layer -->
    <circle cx="560" cy="415" r="6" class="neuron"/>
    <circle cx="560" cy="435" r="6" class="neuron"/>
    <circle cx="560" cy="455" r="6" class="neuron"/>
    <circle cx="560" cy="475" r="6" class="neuron"/>
    
    <!-- Output layer -->
    <circle cx="640" cy="425" r="6" class="neuron"/>
    <circle cx="640" cy="445" r="6" class="neuron"/>
    <circle cx="640" cy="465" r="6" class="neuron"/>
    
    <!-- Connections -->
    <line x1="486" y1="425" x2="554" y2="415" class="connection"/>
    <line x1="486" y1="425" x2="554" y2="435" class="connection"/>
    <line x1="486" y1="425" x2="554" y2="455" class="connection"/>
    <line x1="486" y1="425" x2="554" y2="475" class="connection"/>
    
    <line x1="486" y1="445" x2="554" y2="415" class="connection"/>
    <line x1="486" y1="445" x2="554" y2="435" class="connection"/>
    <line x1="486" y1="445" x2="554" y2="455" class="connection"/>
    <line x1="486" y1="445" x2="554" y2="475" class="connection"/>
    
    <line x1="566" y1="415" x2="634" y2="425" class="connection"/>
    <line x1="566" y1="435" x2="634" y2="425" class="connection"/>
    <line x1="566" y1="455" x2="634" y2="445" class="connection"/>
    <line x1="566" y1="475" x2="634" y2="465" class="connection"/>
  </g>
  
  <text x="690" y="440" class="small-label">Non-linear transformations</text>
  
  <!-- Layer repetition note -->
  <text x="590" y="505" class="small-label" text-anchor="middle" fill="white" font-weight="bold">↓ Repeated N times (e.g., 12-96 layers) ↓</text>
  
  <!-- Normalization -->
  <rect x="420" y="515" width="340" height="40" class="box" rx="5"/>
  <text x="590" y="540" class="small-label" text-anchor="middle">Layer Normalization + Residual Connections</text>
  
  <!-- Final Processing -->
  <rect x="420" y="560" width="340" height="40" class="box" rx="5"/>
  <text x="590" y="585" class="small-label" text-anchor="middle">Output: Contextualized representations</text>
  
  <!-- Arrow to Output -->
  <path d="M 590 605 L 590 635" class="arrow"/>
  
  <!-- Output Layer -->
  <text x="470" y="660" class="subtitle">4. Output Layer</text>
  <rect x="450" y="670" width="280" height="80" class="output-box" rx="5"/>
  <text x="590" y="695" class="label" text-anchor="middle" fill="white">Project to vocabulary</text>
  <text x="590" y="715" class="small-label" text-anchor="middle" fill="white">Probabilities over 50k+ tokens</text>
  <text x="590" y="735" class="small-label" text-anchor="middle" fill="white">Next token: "mat" (45%), "floor" (23%), ...</text>
  
  <!-- Key Components Box -->
  <rect x="850" y="205" width="320" height="350" class="box" rx="5"/>
  <text x="1010" y="230" class="subtitle" text-anchor="middle">Key Components</text>
  
  <!-- Tokenizer explanation -->
  <text x="870" y="260" class="label" font-weight="bold">Tokenizer:</text>
  <text x="870" y="280" class="small-label">• Splits text into subwords</text>
  <text x="870" y="295" class="small-label">• BPE, WordPiece, or SentencePiece</text>
  <text x="870" y="310" class="small-label">• Maps to vocabulary IDs</text>
  
  <!-- Attention explanation -->
  <text x="870" y="340" class="label" font-weight="bold">Attention Weighting:</text>
  <text x="870" y="360" class="small-label">• Determines token relationships</text>
  <text x="870" y="375" class="small-label">• Score = softmax(Q·K^T / √d)</text>
  <text x="870" y="390" class="small-label">• Weighted sum of values</text>
  <text x="870" y="405" class="small-label">• Multi-head for diverse patterns</text>
  
  <!-- Neural Network explanation -->
  <text x="870" y="435" class="label" font-weight="bold">Neural Network:</text>
  <text x="870" y="455" class="small-label">• Stacked transformer layers</text>
  <text x="870" y="470" class="small-label">• Each layer refines context</text>
  <text x="870" y="485" class="small-label">• Parameters: billions of weights</text>
  <text x="870" y="500" class="small-label">• Trained on massive text data</text>
  
  <!-- Training note -->
  <text x="870" y="530" class="label" font-weight="bold">Training:</text>
  <text x="870" y="545" class="small-label">• Predict next token objective</text>
  
  <!-- Data flow note at bottom -->
  <text x="600" y="780" class="small-label" text-anchor="middle" font-style="italic">
    Data flows: Input → Tokens → Embeddings → Attention + Neural Layers → Output Probabilities
  </text>
</svg>