<svg viewBox="0 0 1400 1400" xmlns="http://www.w3.org/2000/svg">
  <!-- Define styles -->
  <defs>
    <style>
      .title { font-family: Arial, sans-serif; font-size: 24px; font-weight: bold; fill: #2c3e50; }
      .subtitle { font-family: Arial, sans-serif; font-size: 16px; font-weight: bold; fill: #34495e; }
      .label { font-family: Arial, sans-serif; font-size: 14px; fill: #2c3e50; }
      .small-label { font-family: Arial, sans-serif; font-size: 12px; fill: #555; }
      .box { fill: #ecf0f1; stroke: #34495e; stroke-width: 2; }
      .tokenizer-box { fill: #3498db; stroke: #2980b9; stroke-width: 2; }
      .embedding-box { fill: #9b59b6; stroke: #8e44ad; stroke-width: 2; }
      .attention-box { fill: #e74c3c; stroke: #c0392b; stroke-width: 2; }
      .neural-box { fill: #2ecc71; stroke: #27ae60; stroke-width: 2; }
      .output-box { fill: #f39c12; stroke: #d68910; stroke-width: 2; }
      .multimodal-box { fill: #1abc9c; stroke: #16a085; stroke-width: 2; }
      .positional-box { fill: #e67e22; stroke: #d35400; stroke-width: 2; }
      .arrow { fill: none; stroke: #34495e; stroke-width: 2; marker-end: url(#arrowhead); }
      .neuron { fill: #3498db; stroke: #2980b9; stroke-width: 1.5; }
      .connection { stroke: #95a5a6; stroke-width: 1; opacity: 0.3; }
      .attention-line { stroke: #e74c3c; stroke-width: 2; opacity: 0.6; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#34495e" />
    </marker>
  </defs>
  
  <!-- Title -->
  <text x="700" y="30" class="title" text-anchor="middle">Modern Multimodal LLM Architecture (Inference)</text>
  
  <!-- Multimodal Inputs -->
  <text x="100" y="80" class="subtitle">Multimodal Inputs</text>
  
  <!-- Text Input -->
  <rect x="50" y="90" width="180" height="50" class="box" rx="5"/>
  <text x="140" y="110" class="small-label" text-anchor="middle" font-weight="bold">Text</text>
  <text x="140" y="128" class="small-label" text-anchor="middle">"The cat sat on"</text>
  
  <!-- Image Input -->
  <rect x="250" y="90" width="180" height="50" class="multimodal-box" rx="5"/>
  <text x="340" y="110" class="small-label" text-anchor="middle" font-weight="bold" fill="white">Image</text>
  <text x="340" y="128" class="small-label" text-anchor="middle" fill="white">[RGB pixels]</text>
  
  <!-- Audio Input -->
  <rect x="450" y="90" width="180" height="50" class="multimodal-box" rx="5"/>
  <text x="540" y="110" class="small-label" text-anchor="middle" font-weight="bold" fill="white">Audio (optional)</text>
  <text x="540" y="128" class="small-label" text-anchor="middle" fill="white">[Waveform data]</text>
  
  <!-- Arrows to Encoders -->
  <path d="M 140 140 L 140 180" class="arrow"/>
  <path d="M 340 140 L 340 180" class="arrow"/>
  <path d="M 540 140 L 540 180" class="arrow"/>
  
  <!-- Modality-Specific Encoders -->
  <text x="100" y="205" class="subtitle">1. Modality Encoders</text>
  
  <!-- Text Tokenizer -->
  <rect x="50" y="215" width="180" height="90" class="tokenizer-box" rx="5"/>
  <text x="140" y="235" class="label" text-anchor="middle" fill="white" font-weight="bold">Text Tokenizer</text>
  <text x="140" y="253" class="small-label" text-anchor="middle" fill="white">BPE/SentencePiece</text>
  <text x="140" y="268" class="small-label" text-anchor="middle" fill="white">["The","cat","sat","on"]</text>
  <text x="140" y="285" class="small-label" text-anchor="middle" fill="white">IDs: [145,2574,3829,319]</text>
  
  <!-- Vision Encoder -->
  <rect x="250" y="215" width="180" height="90" class="multimodal-box" rx="5"/>
  <text x="340" y="235" class="label" text-anchor="middle" fill="white" font-weight="bold">Vision Encoder</text>
  <text x="340" y="253" class="small-label" text-anchor="middle" fill="white">CNN or ViT patches</text>
  <text x="340" y="268" class="small-label" text-anchor="middle" fill="white">16x16 or 14x14 patches</text>
  <text x="340" y="285" class="small-label" text-anchor="middle" fill="white">→ Image tokens</text>
  
  <!-- Audio Encoder -->
  <rect x="450" y="215" width="180" height="90" class="multimodal-box" rx="5"/>
  <text x="540" y="235" class="label" text-anchor="middle" fill="white" font-weight="bold">Audio Encoder</text>
  <text x="540" y="253" class="small-label" text-anchor="middle" fill="white">Spectrogram/Whisper</text>
  <text x="540" y="268" class="small-label" text-anchor="middle" fill="white">Mel-frequency bins</text>
  <text x="540" y="285" class="small-label" text-anchor="middle" fill="white">→ Audio tokens</text>
  
  <!-- Arrows to Embeddings -->
  <path d="M 140 305 L 140 335" class="arrow"/>
  <path d="M 340 305 L 340 335" class="arrow"/>
  <path d="M 540 305 L 540 335" class="arrow"/>
  
  <!-- Unified Embedding Layer -->
  <text x="100" y="360" class="subtitle">2. Unified Embedding Space</text>
  <rect x="50" y="370" width="580" height="70" class="embedding-box" rx="5"/>
  <text x="340" y="395" class="label" text-anchor="middle" fill="white">Project all modalities to shared embedding space (e.g., 4096 dims)</text>
  <text x="340" y="415" class="small-label" text-anchor="middle" fill="white">[Text tokens] + [Image patches] + [Audio frames] → Unified token sequence</text>
  <text x="340" y="430" class="small-label" text-anchor="middle" fill="white">Example: [TXT, TXT, TXT, IMG, IMG, IMG, TXT, ...] all as vectors</text>
  
  <!-- Positional Encoding -->
  <path d="M 340 440 L 340 470" class="arrow"/>
  <rect x="50" y="480" width="580" height="60" class="positional-box" rx="5"/>
  <text x="340" y="505" class="label" text-anchor="middle" fill="white">+ Positional Encodings (RoPE, ALiBi, or Learned)</text>
  <text x="340" y="525" class="small-label" text-anchor="middle" fill="white">Adds position information: sin/cos functions or learned embeddings</text>
  
  <!-- Arrow to Neural Network -->
  <path d="M 340 540 L 340 580" class="arrow"/>
  
  <!-- Neural Network Layers -->
  <text x="340" y="605" class="subtitle">3. Transformer Layers (Stacked N times)</text>
  
  <!-- Layer visualization -->
  <rect x="50" y="615" width="580" height="520" class="neural-box" rx="5"/>
  
  <!-- Single Transformer Block Detail -->
  <text x="340" y="640" class="label" text-anchor="middle" fill="white" font-weight="bold">Transformer Block (repeated 12-96+ times)</text>
  
  <!-- Pre-Layer Norm -->
  <rect x="70" y="650" width="540" height="35" class="box" rx="5"/>
  <text x="340" y="673" class="small-label" text-anchor="middle">Layer Normalization (Pre-LN architecture)</text>
  
  <!-- Multi-Head Attention -->
  <rect x="70" y="695" width="540" height="150" class="attention-box" rx="5"/>
  <text x="340" y="720" class="label" text-anchor="middle" fill="white" font-weight="bold">Multi-Head Self-Attention</text>
  
  <!-- Attention mechanism visualization -->
  <text x="340" y="740" class="small-label" text-anchor="middle" fill="white">Parallel attention heads (8-64 heads)</text>
  
  <!-- Show attention mechanism -->
  <g>
    <text x="90" y="765" class="small-label" fill="white">Q = XW_Q</text>
    <text x="230" y="765" class="small-label" fill="white">K = XW_K</text>
    <text x="370" y="765" class="small-label" fill="white">V = XW_V</text>
    
    <rect x="85" y="775" width="465" height="30" fill="rgba(255,255,255,0.1)" rx="3"/>
    <text x="317" y="795" class="small-label" text-anchor="middle" fill="white">Attention(Q,K,V) = softmax(QK^T/√d_k)V</text>
  </g>
  
  <!-- Attention visualization tokens -->
  <g>
    <circle cx="140" cy="820" r="6" fill="white" opacity="0.9"/>
    <circle cx="220" cy="820" r="6" fill="white" opacity="0.9"/>
    <circle cx="300" cy="820" r="6" fill="white" opacity="0.9"/>
    <circle cx="380" cy="820" r="6" fill="white" opacity="0.9"/>
    <circle cx="460" cy="820" r="6" fill="white" opacity="0.9"/>
    <circle cx="540" cy="820" r="6" fill="white" opacity="0.9"/>
    
    <text x="140" y="838" class="small-label" text-anchor="middle" fill="white">txt</text>
    <text x="220" y="838" class="small-label" text-anchor="middle" fill="white">txt</text>
    <text x="300" y="838" class="small-label" text-anchor="middle" fill="white">img</text>
    <text x="380" y="838" class="small-label" text-anchor="middle" fill="white">img</text>
    <text x="460" y="838" class="small-label" text-anchor="middle" fill="white">txt</text>
    <text x="540" y="838" class="small-label" text-anchor="middle" fill="white">txt</text>
    
    <!-- Attention connections showing cross-modal attention -->
    <line x1="300" y1="820" x2="140" y2="820" class="attention-line" stroke-width="2"/>
    <line x1="300" y1="820" x2="220" y2="820" class="attention-line" stroke-width="3"/>
    <line x1="300" y1="820" x2="380" y2="820" class="attention-line" stroke-width="4"/>
    <line x1="300" y1="820" x2="460" y2="820" class="attention-line" stroke-width="2"/>
  </g>
  
  <!-- Residual Connection -->
  <path d="M 625 770 Q 650 770 650 805 Q 650 840 625 840" class="arrow" stroke-dasharray="5,5"/>
  <text x="660" y="810" class="small-label">Residual</text>
  
  <!-- Post-Attention LayerNorm -->
  <rect x="70" y="855" width="540" height="30" class="box" rx="5"/>
  <text x="340" y="875" class="small-label" text-anchor="middle">Layer Normalization</text>
  
  <!-- Feed-Forward Network -->
  <rect x="70" y="895" width="540" height="120" class="box" rx="5"/>
  <text x="340" y="915" class="label" text-anchor="middle" font-weight="bold">Feed-Forward Network (FFN)</text>
  <text x="340" y="935" class="small-label" text-anchor="middle">Typically: Linear → Activation (GELU/SwiGLU) → Linear</text>
  
  <!-- FFN expansion visualization -->
  <g>
    <text x="120" y="960" class="small-label">Hidden dim (e.g., 4096)</text>
    <rect x="120" y="965" width="100" height="15" fill="#3498db" opacity="0.7"/>
    
    <text x="260" y="960" class="small-label">Expand 4x</text>
    <rect x="260" y="965" width="200" height="15" fill="#e74c3c" opacity="0.7"/>
    
    <text x="500" y="960" class="small-label">Project back</text>
    <rect x="500" y="965" width="100" height="15" fill="#3498db" opacity="0.7"/>
  </g>
  
  <text x="340" y="1000" class="small-label" text-anchor="middle">SwiGLU: x ⊗ σ(xW_1) W_2 (modern) or GELU activation (traditional)</text>
  
  <!-- Residual Connection 2 -->
  <path d="M 625 955 Q 650 955 650 990 Q 650 1025 625 1025" class="arrow" stroke-dasharray="5,5"/>
  <text x="660" y="990" class="small-label">Residual</text>
  
  <!-- Post-FFN LayerNorm -->
  <rect x="70" y="1025" width="540" height="30" class="box" rx="5"/>
  <text x="340" y="1045" class="small-label" text-anchor="middle">Layer Normalization</text>
  
  <!-- Repetition indicator -->
  <text x="340" y="1075" class="small-label" text-anchor="middle" fill="white" font-weight="bold">↓ This block repeats N times (GPT-3: 96 layers, GPT-4: ~120, Claude: undisclosed) ↓</text>
  
  <!-- KV Caching note -->
  <rect x="70" y="1090" width="540" height="40" class="box" rx="5" fill="#f1c40f" stroke="#f39c12"/>
  <text x="340" y="1115" class="small-label" text-anchor="middle">Note: During generation, Key-Value (KV) cache stores past tokens to avoid recomputation</text>
  
  <!-- Arrow to Output -->
  <path d="M 340 1135 L 340 1175" class="arrow"/>
  
  <!-- Output Layer -->
  <text x="100" y="1200" class="subtitle">4. Output Head</text>
  <rect x="50" y="1210" width="580" height="100" class="output-box" rx="5"/>
  <text x="340" y="1235" class="label" text-anchor="middle" fill="white">Final Layer Normalization</text>
  <text x="340" y="1255" class="label" text-anchor="middle" fill="white">Linear projection to vocabulary (50k-250k+ tokens)</text>
  <text x="340" y="1275" class="small-label" text-anchor="middle" fill="white">Logits → Softmax → Probability distribution</text>
  <text x="340" y="1295" class="small-label" text-anchor="middle" fill="white">Next token prediction: "mat" (32%), "floor" (18%), "table" (12%), ...</text>
  
  <!-- Sampling Strategy -->
  <rect x="50" y="1320" width="580" height="50" class="box" rx="5"/>
  <text x="340" y="1340" class="small-label" text-anchor="middle">Sampling: Top-k, Top-p (nucleus), Temperature scaling</text>
  <text x="340" y="1358" class="small-label" text-anchor="middle">Selected token fed back as input for next prediction (autoregressive)</text>
  
  <!-- Key Components Box -->
  <rect x="700" y="90" width="650" height="850" class="box" rx="5"/>
  <text x="1025" y="120" class="subtitle" text-anchor="middle">Modern Architecture Components</text>
  
  <!-- Multimodal Section -->
  <text x="720" y="150" class="label" font-weight="bold">Multimodal Processing:</text>
  <text x="720" y="170" class="small-label">• Vision: Patch embeddings (ViT) or CNN features</text>
  <text x="720" y="185" class="small-label">• Audio: Mel spectrograms → encoder (Whisper-style)</text>
  <text x="720" y="200" class="small-label">• Cross-modal attention enables understanding</text>
  <text x="720" y="215" class="small-label">• Examples: GPT-4V, Claude 3, Gemini</text>
  
  <!-- Positional Encoding -->
  <text x="720" y="245" class="label" font-weight="bold">Positional Encodings:</text>
  <text x="720" y="265" class="small-label">• RoPE (Rotary Position Embedding): relative positions</text>
  <text x="720" y="280" class="small-label">• ALiBi: Attention with Linear Biases</text>
  <text x="720" y="295" class="small-label">• Absolute learned embeddings (older models)</text>
  <text x="720" y="310" class="small-label">• Critical for sequence order understanding</text>
  
  <!-- Attention Details -->
  <text x="720" y="340" class="label" font-weight="bold">Attention Mechanisms:</text>
  <text x="720" y="360" class="small-label">• Multi-Head: 8-128 parallel attention heads</text>
  <text x="720" y="375" class="small-label">• Grouped-Query Attention (GQA): efficient KV sharing</text>
  <text x="720" y="390" class="small-label">• Flash Attention: memory-efficient computation</text>
  <text x="720" y="405" class="small-label">• Causal masking for autoregressive generation</text>
  <text x="720" y="420" class="small-label">• Score = softmax(QK^T/√d_k) × V</text>
  
  <!-- Layer Architecture -->
  <text x="720" y="450" class="label" font-weight="bold">Layer Architecture:</text>
  <text x="720" y="470" class="small-label">• Pre-LN (modern): LayerNorm before sub-layers</text>
  <text x="720" y="485" class="small-label">• Residual connections: x + SubLayer(x)</text>
  <text x="720" y="500" class="small-label">• RMSNorm: simpler than LayerNorm (Llama)</text>
  <text x="720" y="515" class="small-label">• Depth: 12-24 (small), 32-80 (large), 96-120+ (XL)</text>
  
  <!-- Feed-Forward Networks -->
  <text x="720" y="545" class="label" font-weight="bold">Feed-Forward Networks (FFN):</text>
  <text x="720" y="565" class="small-label">• SwiGLU activation (modern): x ⊗ σ(xW₁)W₂</text>
  <text x="720" y="580" class="small-label">• GELU activation (traditional): x·Φ(x)</text>
  <text x="720" y="595" class="small-label">• Typical expansion: 4x hidden dimension</text>
  <text x="720" y="610" class="small-label">• Example: 4096 → 16384 → 4096</text>
  <text x="720" y="625" class="small-label">• MoE (Mixture of Experts): conditional routing</text>
  
  <!-- Context Length -->
  <text x="720" y="655" class="label" font-weight="bold">Context Length:</text>
  <text x="720" y="675" class="small-label">• Original Transformers: 512-2048 tokens</text>
  <text x="720" y="690" class="small-label">• Modern LLMs: 8k-200k+ tokens</text>
  <text x="720" y="705" class="small-label">• Extended context: sparse attention, memory</text>
  <text x="720" y="720" class="small-label">• KV cache optimization for long sequences</text>
  
  <!-- Optimization Techniques -->
  <text x="720" y="750" class="label" font-weight="bold">Optimization Techniques:</text>
  <text x="720" y="770" class="small-label">• KV Caching: store attention keys/values</text>
  <text x="720" y="785" class="small-label">• Quantization: FP16, INT8, INT4 for inference</text>
  <text x="720" y="800" class="small-label">• Model parallelism: tensor/pipeline splitting</text>
  <text x="720" y="815" class="small-label">• Flash Attention: fused kernel operations</text>
  
  <!-- Model Scale -->
  <text x="720" y="845" class="label" font-weight="bold">Model Scale Examples:</text>
  <text x="720" y="865" class="small-label">• GPT-3: 175B params, 96 layers, 12,288 dims</text>
  <text x="720" y="880" class="small-label">• GPT-4: ~1.7T params (rumored), MoE architecture</text>
  <text x="720" y="895" class="small-label">• Llama 3: 8B-70B params, GQA, RoPE, SwiGLU</text>
  <text x="720" y="910" class="small-label">• Claude: Undisclosed architecture details</text>
  
  <!-- Data flow note at bottom -->
  <text x="700" y="970" class="small-label" font-style="italic">
    Modern LLMs use sophisticated architectural innovations:
  </text>
  <text x="700" y="988" class="small-label" font-style="italic">
    • Efficient attention (Flash, GQA) for long contexts
  </text>
  <text x="700" y="1006" class="small-label" font-style="italic">
    • Multimodal fusion for vision, audio, text
  </text>
  <text x="700" y="1024" class="small-label" font-style="italic">
    • Advanced activations (SwiGLU) and normalization (RMSNorm)
  </text>
  <text x="700" y="1042" class="small-label" font-style="italic">
    • Mixture of Experts for sparse, efficient computation
  </text>
  <text x="700" y="1060" class="small-label" font-style="italic">
    • Optimized positional encodings (RoPE, ALiBi) for better generalization
  </text>
</svg>